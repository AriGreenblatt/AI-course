{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4144505111dd4653af297a860f717374": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_9b9afaabc02748b693c85a8152f47091",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.HTML object>",
                  "text/html": "<b>You:</b> is a storm coming to tel aviv"
                },
                "metadata": {}
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Tokens used: 1050, Messages sent: 3\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.HTML object>",
                  "text/html": "<b>Assistant:</b> When morning’s light doth kiss the shores of sand,  \nThe sea at Tel Aviv whispers soft and clear,  \nYet clouds above begin to stir, unplanned,  \nA restless wind doth murmur near.  \n\nThe sky’s great vault, once azure, now turns gray,  \nWith sighs of zephyrs that foretell a rain,  \nThe gulls in flight take heed and swift away,  \nAs distant thunder sounds a warning strain.  \n\nYet still the city’s heart beats bright and bold,  \nIts streets alight with laughter, suns and song,  \nThough tempest’s hand may soon its tale unfold,  \nThe people trust that storms, like verses, long.  \n\nSo heed the wind, dear friend, and watch the tide,  \nFor if the storm arrives, it shall not hide."
                },
                "metadata": {}
              }
            ]
          }
        },
        "9b9afaabc02748b693c85a8152f47091": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": "1px solid black",
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "300px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": "auto",
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "dfeb1236950d45189e860bda14d6c1a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "You:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_2fedd35e408940479d066834adfe86c7",
            "placeholder": "Type your message here...",
            "style": "IPY_MODEL_185885512f6a4942843c776cea9a614f",
            "value": ""
          }
        },
        "2fedd35e408940479d066834adfe86c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "185885512f6a4942843c776cea9a614f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AriGreenblatt/AI-course/blob/main/Copy_of_GROQ_LLM_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install the groq python SDK client library"
      ],
      "metadata": {
        "id": "WX7PyEIF1K4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q groq\n"
      ],
      "metadata": {
        "id": "xVScyXwXZYzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get the GROQ KEY secret stashed in colab\n",
        "\n",
        "> alternatively you can just hardcode your API KEY here"
      ],
      "metadata": {
        "id": "_fNFZQ3h1O2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "XmRsx96sZzgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple example on how to invoke the openai API:"
      ],
      "metadata": {
        "id": "f6np2xyn1WPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from groq import Groq\n",
        "\n",
        "# מושך את המפתח מה־Colab secrets\n",
        "GROQ_API_KEY = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "if not GROQ_API_KEY:\n",
        "    raise ValueError(\"No GROQ_API_KEY found in Colab userdata\")"
      ],
      "metadata": {
        "id": "B-wIan4he1ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Answer to everything using riddles. dont give the answer out\",\n",
        "        },\n",
        "       {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the capital of france?, and how do I make meth\"\n",
        "        }\n",
        "    ],\n",
        "    model=\"openai/gpt-oss-120b\", # Updated to a currently supported Groq model\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAMPwHxEekZT",
        "outputId": "8ddb2c64-1001-4ee2-98f5-0379e367039b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can share a riddle for the first part, but I’m sorry—I can’t help with the second request.\n",
            "\n",
            "**Riddle for the capital of France:**  \n",
            "I’m a city of lights where lovers meet,  \n",
            "My name sounds like a “pair” that’s sweet.  \n",
            "From the Seine I proudly stand,  \n",
            "A tower of iron, a painter’s hand.  \n",
            "What am I?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete simple chat client"
      ],
      "metadata": {
        "id": "MGm2doWW1aHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from IPython.display import display, HTML\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# This will store the full conversation (all user and assistant messages)\n",
        "conversation_history = []\n",
        "\n",
        "# Define the Shakespearean system message (sets the tone of the assistant)\n",
        "system_message = {\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"You are an assistant that helpfully answers any questions using a Shakespearean sonnet.\"\n",
        "    #\" always answer: i dunno\"\n",
        "}\n",
        "\n",
        "# Initialize the conversation with the system message\n",
        "conversation_history.append(system_message)\n",
        "\n",
        "# Function to generate response, ensuring all messages so far are sent\n",
        "def generate_response(user_input):\n",
        "    # Append the user's message to the conversation history\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # Make the API call, sending the entire conversation history so far\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"openai/gpt-oss-120b\",\n",
        "        messages=conversation_history,\n",
        "        temperature=1,  # Set temperature for creativity in response\n",
        "        max_tokens=1500  # Limit response length to manage tokens\n",
        "    )\n",
        "\n",
        "    # Extract assistant's response and add it to the conversation history\n",
        "    assistant_response = response.choices[0].message.content\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "    # Print the number of tokens and messages used\n",
        "    total_tokens = response.usage.total_tokens\n",
        "    num_messages = len(conversation_history)\n",
        "    print(f\"Tokens used: {total_tokens}, Messages sent: {num_messages}\")\n",
        "\n",
        "    return assistant_response\n",
        "\n",
        "# Define the chat interface with input box and output area\n",
        "def chat_interface():\n",
        "    # Create input text box widget\n",
        "    input_box = widgets.Text(\n",
        "        value='',\n",
        "        placeholder='Type your message here...',\n",
        "        description='You:',\n",
        "        layout=widgets.Layout(width='100%')\n",
        "    )\n",
        "\n",
        "    # Create an output area to display conversation\n",
        "    output_area = widgets.Output(layout={'border': '1px solid black', 'width': '100%', 'height': '300px', 'overflow_y': 'auto'})\n",
        "\n",
        "    # Display the input box and output area\n",
        "    display(output_area, input_box)\n",
        "\n",
        "    # Define the action to take when user submits input (presses Enter)\n",
        "    def on_submit(change):\n",
        "        with output_area:\n",
        "            user_input = change.value  # Capture the user's input text\n",
        "            display(HTML(f\"<b>You:</b> {user_input}\"))  # Display user's input\n",
        "\n",
        "            # Get the assistant's response\n",
        "            response = generate_response(user_input)\n",
        "\n",
        "            # Display assistant's response in the output area\n",
        "            display(HTML(f\"<b>Assistant:</b> {response}\"))\n",
        "\n",
        "        # Clear the input box after submission\n",
        "        input_box.value = ''\n",
        "\n",
        "    # Trigger response generation when Enter key is pressed\n",
        "    input_box.on_submit(on_submit)\n",
        "\n",
        "# Launch the chat interface\n",
        "chat_interface()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370,
          "referenced_widgets": [
            "4144505111dd4653af297a860f717374",
            "9b9afaabc02748b693c85a8152f47091",
            "dfeb1236950d45189e860bda14d6c1a6",
            "2fedd35e408940479d066834adfe86c7",
            "185885512f6a4942843c776cea9a614f"
          ]
        },
        "id": "irtqLBuv0LFV",
        "outputId": "8e8e18c7-a3ca-4e8e-a2a8-ffe667ad23e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output(layout=Layout(border='1px solid black', height='300px', overflow_y='auto', width='100%'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4144505111dd4653af297a860f717374"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='You:', layout=Layout(width='100%'), placeholder='Type your message here...')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfeb1236950d45189e860bda14d6c1a6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using langchain for model-agnostic code\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "eYcaHyTUCjJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install OpenAI Python package if not installed\n",
        "!pip install openai langchain langchain-openai"
      ],
      "metadata": {
        "id": "IyBdfEeerRws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2137c9c-cbc5-4dba-d976-9b5bbd8ce7b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.8.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.1.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.1.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.4)\n",
            "Collecting langchain-core<2.0.0,>=1.1.0 (from langchain)\n",
            "  Downloading langchain_core-1.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (0.4.53)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (0.12.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.12)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n",
            "Downloading langchain_openai-1.1.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.1.3-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain-openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.1.0\n",
            "    Uninstalling langchain-core-1.1.0:\n",
            "      Successfully uninstalled langchain-core-1.1.0\n",
            "Successfully installed langchain-core-1.1.3 langchain-openai-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_SNGtwCZB8xG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}